# ğŸ§  Neural Networks from Scratch
A Fully Custom Deep Learning Framework Implemented in Python

## ğŸš€ Overview

This project implements a fully functional neural network from scratch using NumPy without relying on deep learning libraries like TensorFlow or PyTorch. The framework supports:  

âœ… Fully Connected Networks (Dense Layers)

âœ… Convolutional Neural Networks (CNNs)

âœ… Recurrent Neural Networks (RNNs)

âœ… Activation Functions (ReLU, Softmax, Sigmoid, etc.)

âœ… Dropout Regularization

âœ… L1 & L2 Weight Regularization

âœ… Optimizers (SGD, Adam, etc.)

âœ… Custom Loss Functions

âœ… Forward & Backward Propagation

âœ… Model Saving & Loading

This is a great project for understanding how deep learning models work at a fundamental level! 

## ğŸ”§ Installation
__1ï¸âƒ£ Clone the repository__

```
git clone https://github.com/ArjunG19/Neural-Network-from-Scratch
```

__2ï¸âƒ£ Install dependencies__

```
pip install -r requirements.txt
```

## ğŸ—ï¸ Features

__ğŸ“Œ 1. Fully Connected Layers__

The network supports multiple dense layers with flexible neuron counts. Example:

```python
from NeuralNetwork import Layer_Dense

layer1 = Layer_Dense(n_inputs=784, n_neurons=128)
layer2 = Layer_Dense(n_inputs=128, n_neurons=10)
```
__ğŸ“Œ 2. Activation Functions__

Implemented ReLU, Softmax, and Sigmoid activations. Example:

```python
from NeuralNetwork import Activation_ReLU, Activation_Softmax

activation1 = Activation_ReLU()
activation2 = Activation_Softmax()
```
__ğŸ“Œ 3. Dropout Regularization__
Prevent overfitting with dropout layers:

```python
from NeuralNetwork import Layer_Dropout

dropout = Layer_Dropout(rate=0.2)  # Drops 20% of neurons during training
```
__ğŸ“Œ 4. Optimizers (SGD, Adam, etc.)__
Train with different optimizers:

```python
from NeuralNetwork import Optimizer_Adam

optimizer = Optimizer_Adam(learning_rate=0.001,decay=1e-3)
```
__ğŸ“Œ 5. Training & Evaluation__
```python
model.train(X_train, y_train, epochs=10, batch_size=32)
model.evaluate(X_test, y_test)
```

## ğŸ† Results
ğŸ¯ Achieved high accuracy on datasets like MNIST, CIFAR-10, and more!

ğŸ“‰ Loss reduced significantly with proper regularization and dropout.

ğŸ“ˆ CNN outperformed simple dense networks for image classification.

## ğŸ§  Future Enhancements

Implementing LSTMs & GRUs for advanced sequential learning.

Expanding support for custom datasets.

Adding hyperparameter tuning utilities.

